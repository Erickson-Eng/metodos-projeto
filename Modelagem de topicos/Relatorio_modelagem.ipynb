{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>__MÉTODOS NUMÉRICOS__</center>\n",
    "## <center>__PROJETO DA UNIDADE 2__</center>\n",
    "\n",
    "#### <center>__ALUNO:__Erickson Azevêdo</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1. INTRODUÇÃO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto do Processamento de Linguagem Natural (PLN), a modelagem de tópicos é um problema de aprendizado não supervisionado cujo objetivo é encontrar tópicos abstratos em uma coleção de documentos. \n",
    "\n",
    "A modelagem de tópicos é uma abordagem utilizada para compreesão e classificação automática de dados em uma variedade de configurações, e talvez a aplicação canônica seja descobrir a estrutura temática em um corpus de documentos. Vários trabalhos fundamentais tanto em aprendizado de máquina quanto em teoria sugeriram um modelo probabilístico para documentos, em que os documentos surgem como uma combinação convexa de um pequeno numero de vetores de tópicos, cada vetor de tópico sendo uma distribuição em palavras.\n",
    "\n",
    "![Modelagem de topicos](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/09/Screenshot_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "2. DESCRIÇÃO DO PROBLEMA\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como apresentado na imagem anterior o problema da modelagem de tópicos consiste em determinar se a partir de um corpo de texto de muitos documentos é possivel encontrar os tópicos abstratos sobre os quais o texto está falando.\n",
    "Essa abordagem é utilizada em diversas ocasiões como por exemplo para entender feedbacks de usuarios [[1]](https://nkoenig06.github.io/gd-tm-svd.html), descobrir quais tópicos estão sendo relevantes no twitter e varias outras aplicações [[2]](https://towardsdatascience.com/lda-topic-modeling-with-tweets-deff37c0e131).\n",
    "\n",
    "A Modelagem de Tópicos ajuda a destilar as informações no corpus de texto grande em um certo número de tópicos. Os tópicos são grupos de palavras que são *semelhantes* no contexto e são indicativos das informações na coleção de documentos.\n",
    "\n",
    "A estrutura geral da Matriz Documento-Termo para um corpus de texto contendo M documentos e N termos ao todo é mostrada abaixo:\n",
    "\n",
    "![Matriz Documento-Termo](https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FBYWRsHWtmGOUC5N4fwNhMqohMAC3-0693jy9.png&w=1920&q=75)\n",
    "\n",
    "Vamos analisar a representação matricial:\n",
    "\n",
    "- D1, D2, ..., DM são os documentos M.\n",
    "- T1, T2, ..., TN são os N termos\n",
    "\n",
    "Para preencher a Matriz de Termo de Documento, vamos usar a métrica amplamente utilizada – a pontuação TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "3. MÉTODOS APLICADOS À SOLUÇÃO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção, você descreverá que métodos numéricos usará para solucionar o problema acima, explicando como esses métodos funcionam, para que tipos de problemas eles são úteis e, principalmente, porque são úteis para o problema descrito na seção anterior\n",
    "\n",
    "\n",
    "###  Equação de pontuação TF-IDF\n",
    "\n",
    "A pontuação TF-IDF é dada pela seguinte equação:\n",
    "\n",
    "$$ W_{ij} = TF_{ij} * log (\\frac{M}{df_{i}})$$\n",
    "\n",
    "No qual, \n",
    "- $TF_{ij}$ é o número de vezes que o termo  $T_j$ ocorre no documento  $D_i$.\n",
    "- $df_j$ é o número de documentos que contêm o termo $T_j$\n",
    "\n",
    "Um termo que ocorre com frequência em um documento específico e raramente em todo o corpus tem uma pontuação IDF mais alta.\n",
    "\n",
    "### Modelagem de tópicos usando decomposição de valor singular (SVD)\n",
    "\n",
    "O uso de Singular Value Decomposition (SVD) para modelagem de tópicos é explicado na figura abaixo: \n",
    "\n",
    "![SVD](https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FBYWRsHWtmGOUC5N4fwNhMqohMAC3-jtl3jg6.jpeg&w=1920&q=75)\n",
    "\n",
    "A Decomposição de Valor Singular na Matriz de Termo de Documento D fornece as três matrizes a seguir:\n",
    "\n",
    "- A matriz vetorial singular esquerda U . Esta matriz é obtida pela decomposição própria da matriz Gram D.D_T — também chamada de matriz de similaridade de documentos. A i,j-ésima entrada da matriz de similaridade de documentos significa quão similar é o documento i ao documento j.\n",
    "- A matriz de valores singulares S que significam a importância relativa dos tópicos.\n",
    "- A matriz de vetores singulares à direita V_T , que também é chamada de matriz de tópicos do termo. Os tópicos no texto residem ao longo das linhas desta matriz.\n",
    "\n",
    "O SVD pode ser pensado como uma caixa preta que opera em sua Matriz de Termo de documento (DTM) e produz 3 matrizes, U, S e V_T. E os tópicos residem ao longo das linhas da matriz V_T. isto é ilustrado na figura abaixo.\n",
    "![Modelador de tópicos com svd](https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FBYWRsHWtmGOUC5N4fwNhMqohMAC3-s7c3juw.png&w=1920&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "4. IMPLEMENTAÇÃO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etapas\n",
    "1. Importando dependencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessario realizar o download da dependencia caso não possua\n",
    "- python3 -m spacy download en\n",
    "- pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\erickson\\anaconda3\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting pyldavis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: gensim in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (3.6.0)\n",
      "Requirement already satisfied: numexpr in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (2.7.3)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.3.4)\n",
      "Requirement already satisfied: future in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (0.18.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.20.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (2.11.3)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (58.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyldavis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyldavis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyldavis) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from gensim->pyldavis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from jinja2->pyldavis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from scikit-learn->pyldavis) (2.2.0)\n",
      "Building wheels for collected packages: pyldavis, sklearn\n",
      "  Building wheel for pyldavis (PEP 517): started\n",
      "  Building wheel for pyldavis (PEP 517): finished with status 'done'\n",
      "  Created wheel for pyldavis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136900 sha256=ba379081472ede9084d589ec3590b9cf11e69bd30962a396bb48af1716798a11\n",
      "  Stored in directory: c:\\users\\erickson\\appdata\\local\\pip\\cache\\wheels\\57\\a4\\86\\d10c6c2e0bf149fbc0afb0aa5a6528ac35b30a133a0270c477\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=2d3dd74debe43370e3bf0827e106d62aac817798bc4fe4c59201544ecdceced7\n",
      "  Stored in directory: c:\\users\\erickson\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built pyldavis sklearn\n",
      "Installing collected packages: sklearn, funcy, pyldavis\n",
      "Successfully installed funcy-1.17 pyldavis-3.3.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Carregando os dados a partir do CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.Title.str.split(\" \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. A próxima etapa é importar o tfidfVectorizer do scikit-learn para extração dos dados a partir do texto *lista*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "5. CASOS DE USO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando SVD para descobrir tópicos\n",
    "\n",
    "- Baixando dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Realizando uma limpeza nos dados e retirando palavras conectivas entre outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.apply(lambda doc: [lemma.lemmatize(word) for word in doc])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.apply(lambda doc: [word for word in doc if word not in swords])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.filter_extremes(no_below=5, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dct.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bow = data.apply(dct.doc2bow)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(bow, num_topics=20, id2word=dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Aqui é apresentada a matriz de tópicos com sua respectiva importancia para a frase avaliada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(\"Topic_\"+str(i), lda.print_topic(i))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "6. REFERÊNCIAS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [[1]](https://nkoenig06.github.io/gd-tm-svd.html) *Topic Modeling Company Reviews with SVD*, **N. Koenig** - 06/ December/2019\n",
    "- [[2]](https://towardsdatascience.com/lda-topic-modeling-with-tweets-deff37c0e131) *LDA Topic modeling with Tweets*, **Rob Zifchak** - 10/ Jul/2020\n",
    "- [[3]](https://hackernoon.com/advanced-topic-modeling-tutorial-how-to-use-svd-and-nmf-in-python-to-find-topics-in-text) *Advanced Topic Modeling Tutorial: How to Use SVD & NMF in Python*,  **@balapriya_** 16 mar 2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
