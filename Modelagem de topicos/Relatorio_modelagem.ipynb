{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>__MÉTODOS NUMÉRICOS__</center>\n",
    "## <center>__PROJETO DA UNIDADE 2__</center>\n",
    "\n",
    "#### <center>__ALUNO:__Erickson Azevêdo</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "1. INTRODUÇÃO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto do Processamento de Linguagem Natural (PLN), a modelagem de tópicos é um problema de aprendizado não supervisionado cujo objetivo é encontrar tópicos abstratos em uma coleção de documentos. \n",
    "\n",
    "A modelagem de tópicos é uma abordagem utilizada para compreesão e classificação automática de dados em uma variedade de configurações, e talvez a aplicação canônica seja descobrir a estrutura temática em um corpus de documentos. Vários trabalhos fundamentais tanto em aprendizado de máquina quanto em teoria sugeriram um modelo probabilístico para documentos, em que os documentos surgem como uma combinação convexa de um pequeno numero de vetores de tópicos, cada vetor de tópico sendo uma distribuição em palavras.\n",
    "\n",
    "![Modelagem de topicos](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/09/Screenshot_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "2. DESCRIÇÃO DO PROBLEMA\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como apresentado na imagem anterior o problema da modelagem de tópicos consiste em determinar se a partir de um corpo de texto de muitos documentos é possivel encontrar os tópicos abstratos sobre os quais o texto está falando.\n",
    "Essa abordagem é utilizada em diversas ocasiões como por exemplo para entender feedbacks de usuarios [[1]](https://nkoenig06.github.io/gd-tm-svd.html), descobrir quais tópicos estão sendo relevantes no twitter e varias outras aplicações [[2]](https://towardsdatascience.com/lda-topic-modeling-with-tweets-deff37c0e131).\n",
    "\n",
    "A Modelagem de Tópicos ajuda a destilar as informações no corpus de texto grande em um certo número de tópicos. Os tópicos são grupos de palavras que são *semelhantes* no contexto e são indicativos das informações na coleção de documentos.\n",
    "\n",
    "A estrutura geral da Matriz Documento-Termo para um corpus de texto contendo M documentos e N termos ao todo é mostrada abaixo:\n",
    "\n",
    "![Matriz Documento-Termo](https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FBYWRsHWtmGOUC5N4fwNhMqohMAC3-0693jy9.png&w=1920&q=75)\n",
    "\n",
    "Vamos analisar a representação matricial:\n",
    "\n",
    "- D1, D2, ..., DM são os documentos M.\n",
    "- T1, T2, ..., TN são os N termos\n",
    "\n",
    "Para preencher a Matriz de Termo de Documento, vamos usar a métrica amplamente utilizada – a pontuação TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "3. MÉTODOS APLICADOS À SOLUÇÃO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa seção, você descreverá que métodos numéricos usará para solucionar o problema acima, explicando como esses métodos funcionam, para que tipos de problemas eles são úteis e, principalmente, porque são úteis para o problema descrito na seção anterior\n",
    "\n",
    "\n",
    "###  Equação de pontuação TF-IDF\n",
    "\n",
    "A pontuação TF-IDF é dada pela seguinte equação:\n",
    "\n",
    "$$ W_{ij} = TF_{ij} * log (\\frac{M}{df_{i}})$$\n",
    "\n",
    "No qual, \n",
    "- $TF_{ij}$ é o número de vezes que o termo  $T_j$ ocorre no documento  $D_i$.\n",
    "- $df_j$ é o número de documentos que contêm o termo $T_j$\n",
    "\n",
    "Um termo que ocorre com frequência em um documento específico e raramente em todo o corpus tem uma pontuação IDF mais alta.\n",
    "\n",
    "### Modelagem de tópicos usando decomposição de valor singular (SVD)\n",
    "\n",
    "O uso de Singular Value Decomposition (SVD) para modelagem de tópicos é explicado na figura abaixo: \n",
    "\n",
    "![SVD](https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FBYWRsHWtmGOUC5N4fwNhMqohMAC3-jtl3jg6.jpeg&w=1920&q=75)\n",
    "\n",
    "A Decomposição de Valor Singular na Matriz de Termo de Documento D fornece as três matrizes a seguir:\n",
    "\n",
    "- A matriz vetorial singular esquerda U . Esta matriz é obtida pela decomposição própria da matriz Gram D.D_T — também chamada de matriz de similaridade de documentos. A i,j-ésima entrada da matriz de similaridade de documentos significa quão similar é o documento i ao documento j.\n",
    "- A matriz de valores singulares S que significam a importância relativa dos tópicos.\n",
    "- A matriz de vetores singulares à direita V_T , que também é chamada de matriz de tópicos do termo. Os tópicos no texto residem ao longo das linhas desta matriz.\n",
    "\n",
    "O SVD pode ser pensado como uma caixa preta que opera em sua Matriz de Termo de documento (DTM) e produz 3 matrizes, U, S e V_T. E os tópicos residem ao longo das linhas da matriz V_T. isto é ilustrado na figura abaixo.\n",
    "![Modelador de tópicos com svd](https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FBYWRsHWtmGOUC5N4fwNhMqohMAC3-s7c3juw.png&w=1920&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "4. IMPLEMENTAÇÃO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando dependencias e realizando instalações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessario realizar o download da dependencia caso não possua\n",
    "- python3 -m spacy download en\n",
    "- pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\erickson\\anaconda3\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: pyldavis in c:\\users\\erickson\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.3.4)\n",
      "Requirement already satisfied: future in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (0.18.2)\n",
      "Requirement already satisfied: funcy in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.17)\n",
      "Requirement already satisfied: gensim in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (0.24.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (2.11.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (58.0.4)\n",
      "Requirement already satisfied: sklearn in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (0.0)\n",
      "Requirement already satisfied: numexpr in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (2.7.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pyldavis) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyldavis) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyldavis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyldavis) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from gensim->pyldavis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from jinja2->pyldavis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from scikit-learn->pyldavis) (2.2.0)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\erickson\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!pip install pyldavis\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Realizando a leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Sentiment_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>I like eat delicious food. That's I'm cooking ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>This help eating healthy exercise regular basis</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Works great especially going grocery store</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best idea us</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 Best Foods for You</td>\n",
       "      <td>Best way</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64222</th>\n",
       "      <td>Housing-Real Estate &amp; Property</td>\n",
       "      <td>Most ads older many agents ..not much owner po...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.173333</td>\n",
       "      <td>0.486667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64223</th>\n",
       "      <td>Housing-Real Estate &amp; Property</td>\n",
       "      <td>If photos posted portal load, fit purpose. I'm...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.447222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64226</th>\n",
       "      <td>Housing-Real Estate &amp; Property</td>\n",
       "      <td>Dumb app, I wanted post property rent give opt...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.287500</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64227</th>\n",
       "      <td>Housing-Real Estate &amp; Property</td>\n",
       "      <td>I property business got link SMS happy perform...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64230</th>\n",
       "      <td>Housing-Real Estate &amp; Property</td>\n",
       "      <td>Useless app, I searched flats kondapur, Hydera...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.316667</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37427 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  App  \\\n",
       "0               10 Best Foods for You   \n",
       "1               10 Best Foods for You   \n",
       "3               10 Best Foods for You   \n",
       "4               10 Best Foods for You   \n",
       "5               10 Best Foods for You   \n",
       "...                               ...   \n",
       "64222  Housing-Real Estate & Property   \n",
       "64223  Housing-Real Estate & Property   \n",
       "64226  Housing-Real Estate & Property   \n",
       "64227  Housing-Real Estate & Property   \n",
       "64230  Housing-Real Estate & Property   \n",
       "\n",
       "                                       Translated_Review Sentiment  \\\n",
       "0      I like eat delicious food. That's I'm cooking ...  Positive   \n",
       "1        This help eating healthy exercise regular basis  Positive   \n",
       "3             Works great especially going grocery store  Positive   \n",
       "4                                           Best idea us  Positive   \n",
       "5                                               Best way  Positive   \n",
       "...                                                  ...       ...   \n",
       "64222  Most ads older many agents ..not much owner po...  Positive   \n",
       "64223  If photos posted portal load, fit purpose. I'm...  Positive   \n",
       "64226  Dumb app, I wanted post property rent give opt...  Negative   \n",
       "64227  I property business got link SMS happy perform...  Positive   \n",
       "64230  Useless app, I searched flats kondapur, Hydera...  Negative   \n",
       "\n",
       "       Sentiment_Polarity  Sentiment_Subjectivity  \n",
       "0                1.000000                0.533333  \n",
       "1                0.250000                0.288462  \n",
       "3                0.400000                0.875000  \n",
       "4                1.000000                0.300000  \n",
       "5                1.000000                0.300000  \n",
       "...                   ...                     ...  \n",
       "64222            0.173333                0.486667  \n",
       "64223            0.225000                0.447222  \n",
       "64226           -0.287500                0.250000  \n",
       "64227            0.800000                1.000000  \n",
       "64230           -0.316667                0.400000  \n",
       "\n",
       "[37427 rows x 5 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('google.csv')\n",
    "df = df.dropna(subset=['Translated_Review'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpeza de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like eat delicious food. Thats Im cooking food myself, case \"10 Best Foods\" helps lot, also \"Best Before (Shelf Life)\"']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.Translated_Review.values.tolist()\n",
    "# Remove Emails\n",
    "data = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "# Remove new line characters\n",
    "data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenizar\n",
    "\n",
    "Agora queremos tokenizar cada frase em uma lista de palavras , removendo completamente as pontuações e os caracteres desnecessários.\n",
    "\n",
    "Tokenização é o ato de quebrar uma sequência de strings em pedaços como palavras, palavras-chave, frases, símbolos e outros elementos chamados tokens. Os tokens podem ser palavras individuais, frases ou até mesmo frases inteiras. No processo de tokenização , alguns caracteres como sinais de pontuação são descartados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['like', 'eat', 'delicious', 'food', 'thats', 'im', 'cooking', 'food', 'myself', 'case', 'best', 'foods', 'helps', 'lot', 'also', 'best', 'before', 'shelf', 'life']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streamming\n",
    "Stemming é o processo de redução de uma palavra ao seu radical de palavra que se afixa a sufixos e prefixos ou às raízes de palavras conhecidas como lema.\n",
    "\n",
    "A vantagem disso é que conseguimos reduzir o número total de palavras únicas no dicionário. Como resultado, o número de colunas na matriz documento-palavra (criada pelo CountVectorizer na próxima etapa) será mais denso com colunas menores. Você pode esperar que tópicos melhores sejam gerados no final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): #'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat food s m cook food case food help lot shelf life', 'help eat exercise basis']\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy ‘en’ model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'VERB']) #select noun and verb\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Criando uma matriz\n",
    "\n",
    "Para utilizar o LDA o mesmo necessita uma matriz de palavras do documento como a entrada principal.\n",
    "\n",
    "Você pode criar um usando CountVectorizer. No código abaixo, configurei o CountVectorizer para considerar palavras que ocorreram pelo menos 10 vezes (min_df), remover palavras irrelevantes em inglês, converter todas as palavras para minúsculas e uma palavra pode conter números e alfabetos de pelo menos 3 comprimentos para ser qualificado como uma palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,\n",
    "                             stop_words='english',             \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}')\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Criando modelo LDA com sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(learning_method='online', n_components=20, n_jobs=-1,\n",
      "                          random_state=100)\n"
     ]
    }
   ],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=20,               # Number of topics\n",
    "                                      max_iter=10,               \n",
    "# Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          \n",
    "# Random state\n",
    "                                      batch_size=128,            \n",
    "# n docs in each learning iter\n",
    "                                      evaluate_every = -1,       \n",
    "# compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               \n",
    "# Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como queremos descobrir os melhores parâmetros, usamos a Alocação de Dirichlet Latente com o algoritmo de Bayes variacional online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_jobs=-1, random_state=100)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    " evaluate_every=-1, learning_decay=0.7,\n",
    " learning_method='online', learning_offset=10.0,\n",
    " max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
    " n_components=10, n_jobs=-1, perp_tol=0.1,\n",
    " random_state=100, topic_word_prior=None,\n",
    " total_samples=1000000.0, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Diagnosticar o desempenho do modelo com perplexidade e probabilidade de log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -2121624.4223942687\n",
      "Perplexity:  1044.592609481927\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'online',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 20,\n",
      " 'n_jobs': -1,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 100,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Usando GridSearch para determinar o melhor modelo LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
    "             n_topics=None, perp_tol=0.1, random_state=None,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "       fit_params=None, iid=True, n_jobs=1,\n",
    "       param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Tópico dominante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document — Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "# column names\n",
    "topicnames = ['Topic' + str(i) for i in range(best_lda_model.n_components)]\n",
    "# index names\n",
    "docnames = ['Doc' + str(i) for i in range(len(data))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "# Styling\n",
    "def color_green(val):\n",
    " color = 'green' if val > .1 else 'black'\n",
    " return 'color: {col}'.format(col=color)\n",
    "def make_bold(val):\n",
    " weight = 700 if val > .1 else 400\n",
    " return 'font-weight: {weight}'.format(weight=weight)\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. matriz de probabilidade dos tópicos e tópicos dominantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "# View\n",
    "df_topic_keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pegando top 15 palavras chaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inferindo tópicos de acordo com palavras chave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topics = [\"Update Version/Fix Crash Problem\",\"Download/Internet Access\",\"Learn and Share\",\"Card Payment\",\"Notification/Support\", \n",
    "          \"Account Problem\", \"Device/Design/Password\", \"Language/Recommend/Screen Size\", \"Graphic/ Game Design/ Level and Coin\", \"Photo/Search\"]\n",
    "df_topic_keywords[\"Topics\"]=Topics\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Prevendo tópicos usando LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to predict topic for a given text document.\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "def predict_topic(text, nlp=nlp):\n",
    "    global sent_to_words\n",
    "    global lemmatization\n",
    "# Step 1: Clean with simple_preprocess\n",
    "    mytext_2 = list(sent_to_words(text))\n",
    "# Step 2: Lemmatize\n",
    "    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "# Step 3: Vectorize transform\n",
    "    mytext_4 = vectorizer.transform(mytext_3)\n",
    "# Step 4: LDA Transform\n",
    "    topic_probability_scores = best_lda_model.transform(mytext_4)\n",
    "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), 1:14].values.tolist()\n",
    "    \n",
    "    # Step 5: Infer Topic\n",
    "    infer_topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), -1]\n",
    "    \n",
    "    #topic_guess = df_topic_keywords.iloc[np.argmax(topic_probability_scores), Topics]\n",
    "    return infer_topic, topic, topic_probability_scores\n",
    "# Predict the topic\n",
    "mytext = [\"Very Useful in diabetes age 30. I need control sugar. thanks Good deal\"]\n",
    "infer_topic, topic, prob_scores = predict_topic(text = mytext)\n",
    "print(topic)\n",
    "print(infer_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to predict topic for a given text document.\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "def predict_topic(text, nlp=nlp):\n",
    "    global sent_to_words\n",
    "    global lemmatization\n",
    "# Step 1: Clean with simple_preprocess\n",
    "    mytext_2 = list(sent_to_words(text))\n",
    "# Step 2: Lemmatize\n",
    "    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "# Step 3: Vectorize transform\n",
    "    mytext_4 = vectorizer.transform(mytext_3)\n",
    "# Step 4: LDA Transform\n",
    "    topic_probability_scores = best_lda_model.transform(mytext_4)\n",
    "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), 1:14].values.tolist()\n",
    "    \n",
    "    # Step 5: Infer Topic\n",
    "    infer_topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), -1]\n",
    "    \n",
    "    #topic_guess = df_topic_keywords.iloc[np.argmax(topic_probability_scores), Topics]\n",
    "    return infer_topic, topic, topic_probability_scores\n",
    "# Predict the topic\n",
    "mytext = [\"Very Useful in diabetes age 30. I need control sugar. thanks Good deal\"]\n",
    "infer_topic, topic, prob_scores = predict_topic(text = mytext)\n",
    "print(topic)\n",
    "print(infer_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "5. CASOS DE USO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando dependencias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregando os dados a partir do CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processo de Tokenization e Streamming\n",
    "\n",
    "Agora queremos tokenizar cada frase em uma lista de palavras , removendo completamente as pontuações e os caracteres desnecessários.\n",
    "\n",
    "Tokenização é o ato de quebrar uma sequência de strings em pedaços como palavras, palavras-chave, frases, símbolos e outros elementos chamados tokens. Os tokens podem ser palavras individuais, frases ou até mesmo frases inteiras. No processo de tokenização , alguns caracteres como sinais de pontuação são descartados.\n",
    "\n",
    "Stemming é o processo de redução de uma palavra ao seu radical de palavra que se afixa a sufixos e prefixos ou às raízes de palavras conhecidas como lema.\n",
    "\n",
    "A vantagem disso é que conseguimos reduzir o número total de palavras únicas no dicionário. Como resultado, o número de colunas na matriz documento-palavra (criada pelo CountVectorizer na próxima etapa) será mais denso com colunas menores. Você pode esperar que tópicos melhores sejam gerados no final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.Title.str.split(\" \")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.apply(lambda doc: [lemma.lemmatize(word) for word in doc])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.apply(lambda doc: [word for word in doc if word not in swords])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Este módulo implementa o conceito de Dicionário – um mapeamento entre palavras e seus ids inteiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.filter_extremes(no_below=5, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dct.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bow = data.apply(dct.doc2bow)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(bow, num_topics=20, id2word=dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aqui é apresentada a matriz de tópicos com sua respectiva importancia para a frase avaliada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(\"Topic_\"+str(i), lda.print_topic(i))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "6. REFERÊNCIAS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [[1]](https://nkoenig06.github.io/gd-tm-svd.html) *Topic Modeling Company Reviews with SVD*, **N. Koenig** - 06/ December/2019\n",
    "- [[2]](https://towardsdatascience.com/lda-topic-modeling-with-tweets-deff37c0e131) *LDA Topic modeling with Tweets*, **Rob Zifchak** - 10/ Jul/2020\n",
    "- [[3]](https://hackernoon.com/advanced-topic-modeling-tutorial-how-to-use-svd-and-nmf-in-python-to-find-topics-in-text) *Advanced Topic Modeling Tutorial: How to Use SVD & NMF in Python*,  **@balapriya_** 16 mar 2022\n",
    "- [[4]](https://yanlinc.medium.com/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6) *How to generate an LDA Topic Model for Text Analysis*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
